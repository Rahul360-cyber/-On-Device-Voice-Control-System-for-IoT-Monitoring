{"cells":[{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704206782911,"execution_millis":2576,"deepnote_to_be_reexecuted":false,"cell_id":"ce8574c1880b42d89cb7219d2c1bddaa","deepnote_cell_type":"code"},"source":"import sys\n\n\nif sys.version.split()[0] != '3.10.12':\n    print(sys.version)\n    raise RuntimeError('Wrong Python version. Go to ENVIRONMENT settings, Stop machine, Start machine')\n\n\nimport tensorflow\nif tensorflow.__version__ != '2.13.0':\n    raise RuntimeError('Wrong TF version. Go to ENVIRONMENT settings, Stop machine, Start machine')\n\n\nprint('\\nPython version: OK')\nprint('TensorFlow version: OK')","block_group":"92fc04b8e4374266857e39e7d1e6b9d9","execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-02 14:46:23.029029: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2024-01-02 14:46:23.030827: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-02 14:46:23.065978: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-01-02 14:46:23.067010: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2024-01-02 14:46:24.454343: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n\nPython version: OK\nTensorFlow version: OK\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"c19c86272b87403389f596a81bc586f6","deepnote_cell_type":"text-cell-p"},"source":"","block_group":"31faacd114cb4fe4843d26f1c72c7f4b"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704212354287,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"ee3f7604aea440a3b15492be862ffbce","deepnote_cell_type":"code"},"source":"PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.020,\n    'frame_step_in_s': 0.010,\n    'num_mel_bins': 10,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n    'num_coefficients': 40,\n}\n\nTRAINING_ARGS = {\n    'batch_size': 30,\n    'initial_learning_rate': 0.01,\n    'end_learning_rate': 1.e-5,\n    'epochs': 20\n}\nfinal_sparsity = 0.70\nalpha = 0.25","block_group":"c88dc6e970e546c583365dac7ad704ce","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208043550,"execution_millis":10,"deepnote_to_be_reexecuted":false,"cell_id":"3d1bb1302a0e48dfa93c04b0d2bb2188","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nyes_no_train_ds = tf.data.Dataset.list_files('/tmp/yn-train/*')\nyes_no_test_ds = tf.data.Dataset.list_files('/tmp/yn-test/*')","block_group":"b7e8edb3da034b29adb7e63672e7927f","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208045467,"execution_millis":7,"deepnote_to_be_reexecuted":false,"cell_id":"be04c20d0daa42dcb6abe4e52c11e836","deepnote_cell_type":"code"},"source":"num_train_files = len(yes_no_train_ds)\nnum_test_files = len(yes_no_test_ds)\n\nprint('Training set size:', num_train_files)\nprint('Test set size:', num_test_files)","block_group":"35d435e1ee8346a0b2e8a34f73f60667","execution_count":null,"outputs":[{"name":"stdout","text":"Training set size: 1600\nTest set size: 200\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208047735,"execution_millis":378,"deepnote_to_be_reexecuted":false,"cell_id":"829d5309527e4b37a349a99c14d24d6b","deepnote_cell_type":"code"},"source":"from preprocessing import LABELS\nfrom preprocessing import AudioReader\nfrom preprocessing import MelSpectrogram\nfrom preprocessing import MFCC\n\naudio_reader = AudioReader(tf.int16, 16000)\nmfcc_processor = MFCC(**PREPROCESSING_ARGS)\n\ndef prepare_for_training(feature, label):\n    feature = tf.expand_dims(feature, -1)\n    label_id = tf.argmax(label == LABELS)\n\n    return feature, label_id\n\n\n\nbatch_size = TRAINING_ARGS['batch_size']\nepochs = TRAINING_ARGS['epochs']\n\ntrain_ds = (yes_no_train_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mfcc_processor.get_mfccs_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size)\n            .cache())\ntest_ds = (yes_no_test_ds\n            .map(audio_reader.get_audio_and_label)\n            .map(mfcc_processor.get_mfccs_and_label)\n            .map(prepare_for_training)\n            .batch(batch_size))","block_group":"c295635b0f714d6598d2f44f578e5ffd","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208051014,"execution_millis":419,"deepnote_to_be_reexecuted":false,"cell_id":"d4760347d1a84f4a9111bc463d740209","deepnote_cell_type":"code"},"source":"for example_batch, example_labels in train_ds.take(1):\n  print('Batch Shape:', example_batch.shape)\n  print('Data Shape:', example_batch.shape[1:])\n  print('Labels:', example_labels)","block_group":"9c77a26dfdcf4bb6a79ae2628330b8df","execution_count":null,"outputs":[{"name":"stdout","text":"Batch Shape: (30, 99, 10, 1)\nData Shape: (99, 10, 1)\nLabels: tf.Tensor([1 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 0 1 1 0 0 1 0 1], shape=(30,), dtype=int64)\n2024-01-02 15:07:31.412715: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"a986f65cb9eb44f2bef1849aa1e6135d","deepnote_cell_type":"text-cell-h1"},"source":"# FINAL MODEL","block_group":"9dc4953e1d5342f6b74db7b075788574"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208061122,"execution_millis":58,"deepnote_to_be_reexecuted":false,"cell_id":"a4432d9a1977450d93abf177ed42a7ea","deepnote_cell_type":"code"},"source":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=example_batch.shape[1:]),\n    tf.keras.layers.Conv2D(filters=32, kernel_size=[3, 3], strides=[2, 2], use_bias=False, padding='valid'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Conv2D(filters=16, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], use_bias=False, padding='same'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.ReLU(),\n    tf.keras.layers.GlobalAveragePooling2D(),\n    tf.keras.layers.Dense(units=len(LABELS)),\n    tf.keras.layers.Softmax()\n])","block_group":"97896cadb0a549c1bfc2a0f24e64c954","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"52f9a8bca4d94e4aa4ed9e969740521f","deepnote_cell_type":"text-cell-h1"},"source":"# SETUP MAGNITUDE BASED WEIGHTS PRUNING","block_group":"6c371be19800452d8e1224781af6a82f"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208065038,"execution_millis":140,"deepnote_to_be_reexecuted":false,"cell_id":"c8c209e6c5f645afb7aa67b63fb58d8c","deepnote_cell_type":"code"},"source":"import tensorflow_model_optimization as tfmot\n\nprune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n\nbegin_step = int(len(train_ds) * epochs * 0.2)\nend_step = int(len(train_ds) * epochs)\n\npruning_params = {\n    'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n        initial_sparsity=0.20,\n        final_sparsity=final_sparsity,\n        begin_step=begin_step,\n        end_step=end_step\n    )\n}\n\nmodel_for_pruning = prune_low_magnitude(model, **pruning_params)","block_group":"e947492d93974475b1b4c93de163b6b6","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208069185,"execution_millis":96,"deepnote_to_be_reexecuted":false,"cell_id":"7e202ee714b5404f8fecef95b9c3347e","deepnote_cell_type":"code"},"source":"model_for_pruning.summary()","block_group":"51d8e8ea7af04577b9ed05d5da851e5c","execution_count":null,"outputs":[{"name":"stdout","text":"Model: \"sequential_3\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n prune_low_magnitude_conv2d  (None, 49, 4, 32)         578       \n _10 (PruneLowMagnitude)                                         \n                                                                 \n prune_low_magnitude_batch_  (None, 49, 4, 32)         129       \n normalization_10 (PruneLow                                      \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_re_lu_  (None, 49, 4, 32)         1         \n 10 (PruneLowMagnitude)                                          \n                                                                 \n prune_low_magnitude_conv2d  (None, 49, 4, 16)         9218      \n _11 (PruneLowMagnitude)                                         \n                                                                 \n prune_low_magnitude_batch_  (None, 49, 4, 16)         65        \n normalization_11 (PruneLow                                      \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_re_lu_  (None, 49, 4, 16)         1         \n 11 (PruneLowMagnitude)                                          \n                                                                 \n prune_low_magnitude_conv2d  (None, 49, 4, 16)         4610      \n _12 (PruneLowMagnitude)                                         \n                                                                 \n prune_low_magnitude_batch_  (None, 49, 4, 16)         65        \n normalization_12 (PruneLow                                      \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_re_lu_  (None, 49, 4, 16)         1         \n 12 (PruneLowMagnitude)                                          \n                                                                 \n prune_low_magnitude_conv2d  (None, 49, 4, 32)         9218      \n _13 (PruneLowMagnitude)                                         \n                                                                 \n prune_low_magnitude_batch_  (None, 49, 4, 32)         129       \n normalization_13 (PruneLow                                      \n Magnitude)                                                      \n                                                                 \n prune_low_magnitude_re_lu_  (None, 49, 4, 32)         1         \n 13 (PruneLowMagnitude)                                          \n                                                                 \n prune_low_magnitude_global  (None, 32)                1         \n _average_pooling2d_3 (Prun                                      \n eLowMagnitude)                                                  \n                                                                 \n prune_low_magnitude_dense_  (None, 2)                 132       \n 3 (PruneLowMagnitude)                                           \n                                                                 \n prune_low_magnitude_softma  (None, 2)                 1         \n x_3 (PruneLowMagnitude)                                         \n                                                                 \n=================================================================\nTotal params: 24150 (94.39 KB)\nTrainable params: 12066 (47.13 KB)\nNon-trainable params: 12084 (47.26 KB)\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208073190,"execution_millis":46067,"deepnote_to_be_reexecuted":false,"cell_id":"ca2302ff16424003bfdb93c0fd7ffa5c","deepnote_cell_type":"code"},"source":"loss = tf.losses.SparseCategoricalCrossentropy(from_logits=False)\ninitial_learning_rate = TRAINING_ARGS['initial_learning_rate']\nend_learning_rate = TRAINING_ARGS['end_learning_rate']\n\nlinear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n    initial_learning_rate=initial_learning_rate,\n    end_learning_rate=end_learning_rate,\n    decay_steps=len(train_ds) * epochs,\n)\noptimizer = tf.optimizers.Adam(learning_rate=linear_decay)\nmetrics = [tf.metrics.SparseCategoricalAccuracy()]\ncallbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\nmodel_for_pruning.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n\nhistory = model_for_pruning.fit(train_ds, epochs=epochs, callbacks=callbacks)","block_group":"3323bb90ce99460e9c2bb753a54628e2","execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/20\n54/54 [==============================] - 18s 283ms/step - loss: 0.3898 - sparse_categorical_accuracy: 0.8425\nEpoch 2/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.2318 - sparse_categorical_accuracy: 0.9181\nEpoch 3/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.1637 - sparse_categorical_accuracy: 0.9406\nEpoch 4/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.1326 - sparse_categorical_accuracy: 0.9506\nEpoch 5/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.1029 - sparse_categorical_accuracy: 0.9650\nEpoch 6/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0857 - sparse_categorical_accuracy: 0.9719\nEpoch 7/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0704 - sparse_categorical_accuracy: 0.9762\nEpoch 8/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0531 - sparse_categorical_accuracy: 0.9850\nEpoch 9/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.0448 - sparse_categorical_accuracy: 0.9856\nEpoch 10/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0374 - sparse_categorical_accuracy: 0.9925\nEpoch 11/20\n54/54 [==============================] - 1s 23ms/step - loss: 0.0346 - sparse_categorical_accuracy: 0.9919\nEpoch 12/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0438 - sparse_categorical_accuracy: 0.9862\nEpoch 13/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.0349 - sparse_categorical_accuracy: 0.9925\nEpoch 14/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.0313 - sparse_categorical_accuracy: 0.9931\nEpoch 15/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.0245 - sparse_categorical_accuracy: 0.9950\nEpoch 16/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0307 - sparse_categorical_accuracy: 0.9931\nEpoch 17/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0230 - sparse_categorical_accuracy: 0.9969\nEpoch 18/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.0247 - sparse_categorical_accuracy: 0.9956\nEpoch 19/20\n54/54 [==============================] - 1s 22ms/step - loss: 0.0207 - sparse_categorical_accuracy: 0.9962\nEpoch 20/20\n54/54 [==============================] - 1s 21ms/step - loss: 0.0200 - sparse_categorical_accuracy: 0.9962\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208123993,"execution_millis":14,"deepnote_to_be_reexecuted":false,"cell_id":"e3f98898a6da4e16a258b07024a7461f","deepnote_cell_type":"code"},"source":"import numpy as np\n\n\nfor layer in model_for_pruning.layers:\n    if isinstance(layer, tf.keras.layers.Wrapper):\n        weights = layer.trainable_weights\n    else:\n        weights = layer.weights\n    for weight in weights:        \n        weight_size = weight.numpy().size\n        zero_num = np.count_nonzero(weight == 0)\n        print(\n            f'{weight.name}: {zero_num/weight_size:.2%} sparsity ',\n            f'({zero_num}/{weight_size})',\n        )","block_group":"960d74c9cf604e5795dff26cfa45ab46","execution_count":null,"outputs":[{"name":"stdout","text":"conv2d_10/kernel:0: 70.14% sparsity  (202/288)\nbatch_normalization_10/gamma:0: 0.00% sparsity  (0/32)\nbatch_normalization_10/beta:0: 0.00% sparsity  (0/32)\nconv2d_11/kernel:0: 69.99% sparsity  (3225/4608)\nbatch_normalization_11/gamma:0: 0.00% sparsity  (0/16)\nbatch_normalization_11/beta:0: 0.00% sparsity  (0/16)\nconv2d_12/kernel:0: 69.97% sparsity  (1612/2304)\nbatch_normalization_12/gamma:0: 0.00% sparsity  (0/16)\nbatch_normalization_12/beta:0: 0.00% sparsity  (0/16)\nconv2d_13/kernel:0: 69.99% sparsity  (3225/4608)\nbatch_normalization_13/gamma:0: 0.00% sparsity  (0/32)\nbatch_normalization_13/beta:0: 0.00% sparsity  (0/32)\ndense_3/kernel:0: 70.31% sparsity  (45/64)\ndense_3/bias:0: 0.00% sparsity  (0/2)\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208127609,"execution_millis":2208,"deepnote_to_be_reexecuted":false,"cell_id":"a1d9c3f7f7b8456b833aa8771523c4cf","deepnote_cell_type":"code"},"source":"test_loss, test_accuracy = model_for_pruning.evaluate(test_ds)","block_group":"a02aba630a12498985f71cf5f548d310","execution_count":null,"outputs":[{"name":"stdout","text":"7/7 [==============================] - 2s 265ms/step - loss: 0.0457 - sparse_categorical_accuracy: 0.9900\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208151074,"execution_millis":7,"deepnote_to_be_reexecuted":false,"cell_id":"875395a8dcea4b92b4bb8cb6a865d702","deepnote_cell_type":"code"},"source":"training_loss = history.history['loss'][-1]\ntraining_accuracy = history.history['sparse_categorical_accuracy'][-1]\n\n\nprint(f'Training Loss: {training_loss:.4f}')\nprint(f'Training Accuracy: {training_accuracy*100.:.2f}%')\nprint()\nprint(f'Test Loss: {test_loss:.4f}')\nprint(f'Test Accuracy: {test_accuracy*100.:.2f}%')","block_group":"fda5d1c1d23b4a11ad03458ce89726bf","execution_count":null,"outputs":[{"name":"stdout","text":"Training Loss: 0.0200\nTraining Accuracy: 99.62%\n\nTest Loss: 0.0457\nTest Accuracy: 99.00%\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"d2a6cc3112a34a1e9e80c2cee150b859","deepnote_cell_type":"text-cell-h1"},"source":"# save my model","block_group":"7ebd716ad018462f9cc221b0cedb7098"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208216298,"execution_millis":5026,"deepnote_to_be_reexecuted":false,"cell_id":"96315b7b76f5433c814f53682624dfd8","deepnote_cell_type":"code"},"source":"import os\nfrom time import time\n\ntimestamp = int(time())\n\nsaved_model_dir = f'./saved_models/{timestamp}'\nif not os.path.exists(saved_model_dir):\n    os.makedirs(saved_model_dir)\nmodel.save(saved_model_dir)","block_group":"0640fbde14f14eb486791d5b9836edaa","execution_count":null,"outputs":[{"name":"stdout","text":"WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nWARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\nINFO:tensorflow:Assets written to: ./saved_models/1704208216/assets\nINFO:tensorflow:Assets written to: ./saved_models/1704208216/assets\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208227448,"execution_millis":281,"deepnote_to_be_reexecuted":false,"cell_id":"a67d60f3e47f44c0967c3b197b24f71a","deepnote_cell_type":"code"},"source":"!ls saved_models","block_group":"990501a4ea334c588170ffc501cd9b8a","execution_count":null,"outputs":[{"name":"stdout","text":"1703113800  1703194192\t1703371682  1703896291\t1704207759  ref_model\n1703193655  1703255334\t1703670719  1703938625\t1704208216\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"cd6644228b01406a9a3dd52274d21d1d","deepnote_cell_type":"text-cell-h1"},"source":"# save hyperparameters and results","block_group":"d6eefc66867145b7b165a171fec56231"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208236777,"execution_millis":261,"deepnote_to_be_reexecuted":false,"cell_id":"2cf38792f5764aa79a9a8520bd07b600","deepnote_cell_type":"code"},"source":"import pandas as pd\n\noutput_dict = {\n    'timestamp': timestamp,\n    **PREPROCESSING_ARGS,\n    **TRAINING_ARGS,\n    'final_sparsity': final_sparsity,\n    'test_accuracy': test_accuracy\n}\n\ndf = pd.DataFrame([output_dict])\n\noutput_path='./spectrogram_wp_results.csv'\ndf.to_csv(output_path, mode='a', header=not os.path.exists(output_path), index=False)","block_group":"1dcf18e4cc314cf28cc92bcb96955838","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"1532288bc7184a2f97ef896dce2fa463","deepnote_cell_type":"text-cell-h1"},"source":"# create tflite version of the model","block_group":"0a85fad0e9b64160a86ebae95d328c7a"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208416057,"execution_millis":2484,"deepnote_to_be_reexecuted":false,"cell_id":"3b09a93a39384b499f30b18645a3ed86","deepnote_cell_type":"code"},"source":"MODEL_NAME = \"1704208216\"\nconverter = tf.lite.TFLiteConverter.from_saved_model(f'./saved_models/{MODEL_NAME}')\ntflite_model = converter.convert()","block_group":"816eac1e9ee84c4e8b93a9ec774a0de6","execution_count":null,"outputs":[{"name":"stderr","text":"2024-01-02 15:13:37.670539: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n2024-01-02 15:13:37.670574: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n2024-01-02 15:13:37.841842: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: ./saved_models/1704208216\n2024-01-02 15:13:38.014460: I tensorflow/cc/saved_model/reader.cc:91] Reading meta graph with tags { serve }\n2024-01-02 15:13:38.014495: I tensorflow/cc/saved_model/reader.cc:132] Reading SavedModel debug info (if present) from: ./saved_models/1704208216\n2024-01-02 15:13:38.018879: I tensorflow/cc/saved_model/loader.cc:231] Restoring SavedModel bundle.\n2024-01-02 15:13:38.435259: I tensorflow/cc/saved_model/loader.cc:215] Running initialization op on SavedModel bundle at path: ./saved_models/1704208216\n2024-01-02 15:13:38.453258: I tensorflow/cc/saved_model/loader.cc:314] SavedModel load for tags { serve }; Status: success: OK. Took 611430 microseconds.\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208421162,"execution_millis":8,"deepnote_to_be_reexecuted":false,"cell_id":"ebc777a9977b4f569cb2b9e7f9154e43","deepnote_cell_type":"code"},"source":"import os\ntflite_models_dir = './tflite_models'\nif not os.path.exists(tflite_models_dir):\n    os.makedirs(tflite_models_dir)","block_group":"84226a26ec274a3faf2e3a077543dcd0","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208423805,"execution_millis":13,"deepnote_to_be_reexecuted":false,"cell_id":"f73a9723674147f9a974a2b4984adb55","deepnote_cell_type":"code"},"source":"tflite_model_name = os.path.join(tflite_models_dir, f'{MODEL_NAME}.tflite')\ntflite_model_name","block_group":"4be9b5fc11c64e18a467aae08a757f21","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"'./tflite_models/1704208216.tflite'"},"metadata":{}}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208427199,"execution_millis":232,"deepnote_to_be_reexecuted":false,"cell_id":"7cf6d4dc7d9544758d53400c4a4ad837","deepnote_cell_type":"code"},"source":"with open(tflite_model_name, 'wb') as fp:\n    fp.write(tflite_model)","block_group":"acc0e65c86fd41a4bf9e2e42d8158cfa","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"e704961238bb43c997b97a0bc0ff3130","deepnote_cell_type":"text-cell-h1"},"source":"# converting file into zip file","block_group":"c358d992eb3041a8b4196a13de9ca684"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208430449,"execution_millis":397,"deepnote_to_be_reexecuted":false,"cell_id":"e04e2f8ef30e43a58d2fe41cc57e7dd7","deepnote_cell_type":"code"},"source":"import zipfile\n\nwith zipfile.ZipFile(f'{tflite_model_name}.zip', 'w', compression=zipfile.ZIP_DEFLATED) as f:\n    f.write(tflite_model_name)","block_group":"5202d990308744d78f58f344b1569c43","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"52da8c2749214a54bb822acbdba5d6f3","deepnote_cell_type":"text-cell-h1"},"source":"# model size","block_group":"3af480de355f4c36be5c3443bab84b05"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704208466666,"execution_millis":160,"deepnote_to_be_reexecuted":false,"cell_id":"cd5a05a98fed4b0f91e6eae6ef4e54ac","deepnote_cell_type":"code"},"source":"\ntflite_size = os.path.getsize(tflite_model_name) / 1024.0\nzipped_size = os.path.getsize(f'{tflite_model_name}.zip') / 1024.0\n\nprint(f'Original tflite size (not pruned model): {tflite_size:.3f} KB')\nprint(f'Zipped tflite size (pruned model): {zipped_size:.3f} KB')\n","block_group":"e4f7b8722b454682a5f75c1cb077326d","execution_count":null,"outputs":[{"name":"stdout","text":"Original tflite size (not pruned model): 50.367 KB\nOriginal tflite size (pruned model): 50.367 KB\nZipped tflite size (not pruned model): 20.093 KB\nZipped tflite size (pruned model): 20.093 KB\n","output_type":"stream"}]},{"cell_type":"markdown","metadata":{"formattedRanges":[],"cell_id":"216f076febc64ce1a62b66028f2ad54c","deepnote_cell_type":"text-cell-h1"},"source":"# latency","block_group":"adf7447088b046c29e65c1abda92fd46"},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704212455048,"execution_millis":1617,"deepnote_to_be_reexecuted":false,"cell_id":"305ef9972e734571b536125a93cb49ed","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport os\nfrom preprocessing import MelSpectrogram\nimport numpy as np\nfrom time import time\n\nREF_PREPROCESSING_ARGS = {\n    'sampling_rate': 16000,\n    'frame_length_in_s': 0.04,\n    'frame_step_in_s': 0.02,\n    'num_mel_bins': 40,\n    'lower_frequency': 20,\n    'upper_frequency': 4000,\n}\n\nmel_spec_processor = MelSpectrogram(**REF_PREPROCESSING_ARGS)\ninterpreter = tf.lite.Interpreter(model_path='tflite_models/ref_model.tflite')\ninterpreter.allocate_tensors()\n\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\n\naudio = tf.random.normal((16000,))\n\nref_latencies = []\n\nfor i in range(100):\n    start_preprocess = time()\n\n    log_mel_spectrogram = mel_spec_processor.get_mel_spec(audio)\n    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, 0)\n    log_mel_spectrogram = tf.expand_dims(log_mel_spectrogram, -1)\n    interpreter.set_tensor(input_details[0]['index'], log_mel_spectrogram)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n    ref_latencies.append(end_inference - start_preprocess)\n\nmedian_ref_latency = np.median(ref_latencies)\nmedian_ref1_latency = 1000* median_ref_latency\nprint(median_ref1_latency)","block_group":"cfe8a2c486b241228f79659d1e7d9fa4","execution_count":null,"outputs":[{"name":"stdout","text":"16.301393508911133\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704212483754,"execution_millis":1096,"deepnote_to_be_reexecuted":false,"cell_id":"61eff6134fc046f3a64223862ec48071","deepnote_cell_type":"code"},"source":"import tensorflow as tf\nimport os\nfrom preprocessing import MelSpectrogram\nfrom preprocessing import MFCC\nimport numpy as np\nfrom time import time\n\nmfcc_processor = MFCC(**PREPROCESSING_ARGS)\ninterpreter = tf.lite.Interpreter(model_path=r'/work/tflite_models/1704208216.tflite')\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\naudio = tf.random.normal((16000,))\nOPT4_latencies = []\n\nfor i in range(100):\n    start_preprocess = time()\n\n    mfcc_features = mfcc_processor.get_mfccs(audio)\n        #mfcc_features = tf.reshape(mfcc_features, input_details[0]['shape'])\n    mfcc_features = tf.expand_dims(mfcc_features, 0)  # Add the batch dimension\n    mfcc_features = tf.expand_dims(mfcc_features, -1)\n    interpreter.set_tensor(input_details[0]['index'],mfcc_features)\n    interpreter.invoke()\n    output = interpreter.get_tensor(output_details[0]['index'])\n\n    end_inference = time()\n\n\n    OPT4_latencies.append(end_inference - start_preprocess)\n\nmedian_OPT4_latency = np.median(OPT4_latencies)\nmedian_OPT4_latency = 1000*median_OPT4_latency\nprint(median_OPT4_latency)","block_group":"878400140fd24a3c82859a9ae466123b","execution_count":null,"outputs":[{"name":"stdout","text":"8.942604064941406\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"source_hash":null,"execution_start":1704213135435,"execution_millis":6,"deepnote_to_be_reexecuted":false,"cell_id":"1a8fe69f73574286a1a0c47310c7bf44","deepnote_cell_type":"code"},"source":"TOTAL_LATENCY_SAVING = 100 * (median_ref1_latency - median_OPT4_latency) / median_ref1_latency\nprint(TOTAL_LATENCY_SAVING)","block_group":"69859fae5cb1402984a29d7a3fe6f90a","execution_count":null,"outputs":[{"name":"stdout","text":"45.142088251210275\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=eef6af2f-fd81-4b1e-aa00-f5faf5eb94b7' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"f55b4bca80534785a485864e115a6dc1","deepnote_execution_queue":[]}}